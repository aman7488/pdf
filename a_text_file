package com.example.kerberosHive;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.*;
import java.util.Properties;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.security.UserGroupInformation;

public class HiveKerberosConnection {
    private static final String JDBC_URL = "jdbc:hive2://uklvadhdp006a.pi.dev.net:12181,uklvadhdp007a.pi.dev.net:12181,uklvadhdp008a.pi.dev.net:12181/default;password=hive;principal=hive/_HOST@ZONE1.SCBDEV.NET;serviceDiscoveryMode=zooKeeper;ssl=true;" +
            "sslTrustStore=C:/Users/2010170/Rotation%202/SharePoint%20POC/cm-auto-global_truststore.jks;user=hive;zooKeeperNamespace=hiveserver2";
    private static final String KEYTAB_PATH = "C:/Users/2010170/Rotation 2/SharePoint POC/g.fmoappdev.001.keytab";
    private static final String PRINCIPAL = "g.fmoappdev.001@ZONE1.SCBDEV.NET";
    private static final String HIVE_DRIVER = "org.apache.hive.jdbc.HiveDriver";
    private Connection connection;

    public HiveKerberosConnection() {
        setupKerberos();
    }

    private void setupKerberos() {
        try {
            System.setProperty("java.security.krb5.conf", "C:/Users/2010170/Rotation 2/SharePoint POC/krb5.conf");
            Configuration conf = new Configuration();
            conf.set("hadoop.security.authentication", "kerberos");
            conf.set("hadoop.security.authorization", "true");
            UserGroupInformation.setConfiguration(conf);
            UserGroupInformation.loginUserFromKeytab(PRINCIPAL, KEYTAB_PATH);
            System.out.println("Kerberos authentication successful for: " + PRINCIPAL);
        } catch (Exception e) {
            System.err.println("Kerberos authentication failed: " + e.getMessage());
            e.printStackTrace();
            throw new RuntimeException("Failed to authenticate with Kerberos", e);
        }
    }

    /**
     * Establish connection to Hive
     */
    public void connect() {
        try {
            Class.forName(HIVE_DRIVER);
            Properties props = new Properties();
            connection = DriverManager.getConnection(JDBC_URL, props);
            System.out.println("Connected to Hive successfully!");
        } catch (ClassNotFoundException e) {
            System.err.println("Hive JDBC driver not found: " + e.getMessage());
            throw new RuntimeException("Hive driver not found", e);
        } catch (SQLException e) {
            System.err.println("Failed to connect to Hive: " + e.getMessage());
            e.printStackTrace();
            throw new RuntimeException("Failed to connect to Hive", e);
        }
    }

    /**
     * Execute SELECT queries that return data
     */
    public void executeQuery(String sql) {
        if (connection == null) {
            throw new IllegalStateException("Not connected to Hive. Call connect() first.");
        }
        try (Statement stmt = connection.createStatement();
             ResultSet rs = stmt.executeQuery(sql)) {
            ResultSetMetaData metaData = rs.getMetaData();
            int columnCount = metaData.getColumnCount();
            for (int i = 1; i <= columnCount; i++) {
                System.out.print(metaData.getColumnName(i));
                if (i < columnCount) System.out.print("\t");
            }
            System.out.println();
            for (int i = 1; i <= columnCount; i++) {
                System.out.print("----------");
                if (i < columnCount) System.out.print("\t");
            }
            System.out.println();
            while (rs.next()) {
                for (int i = 1; i <= columnCount; i++) {
                    System.out.print(rs.getString(i));
                    if (i < columnCount) System.out.print("\t");
                }
                System.out.println();
            }
        } catch (SQLException e) {
            System.err.println("Error executing query: " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * Execute INSERT, UPDATE, DELETE statements
     * Returns the number of rows affected
     */
    public int executeUpdate(String sql) {
        if (connection == null) {
            throw new IllegalStateException("Not connected to Hive. Call connect() first.");
        }

        try (Statement stmt = connection.createStatement()) {
            System.out.println("Executing UPDATE/INSERT statement...");
            System.out.println("SQL: " + sql.substring(0, Math.min(100, sql.length())) + "...");

            long startTime = System.currentTimeMillis();
            int rowsAffected = stmt.executeUpdate(sql);
            long endTime = System.currentTimeMillis();

            System.out.println("Statement executed successfully!");
            System.out.println("Rows affected: " + rowsAffected);
            System.out.println("Execution time: " + (endTime - startTime) + " ms");

            return rowsAffected;

        } catch (SQLException e) {
            System.err.println("Error executing update statement: " + e.getMessage());
            e.printStackTrace();
            return -1;
        }
    }

    /**
     * Execute DDL statements (CREATE, ALTER, DROP, etc.)
     * Returns true if execution was successful
     */
    public boolean executeDDL(String sql) {
        if (connection == null) {
            throw new IllegalStateException("Not connected to Hive. Call connect() first.");
        }

        try (Statement stmt = connection.createStatement()) {
            System.out.println("Executing DDL statement...");
            System.out.println("SQL: " + sql.substring(0, Math.min(100, sql.length())) + "...");

            long startTime = System.currentTimeMillis();
            boolean result = stmt.execute(sql);
            long endTime = System.currentTimeMillis();

            System.out.println("DDL statement executed successfully!");
            System.out.println("Execution time: " + (endTime - startTime) + " ms");

            return true;

        } catch (SQLException e) {
            System.err.println("Error executing DDL statement: " + e.getMessage());
            e.printStackTrace();
            return false;
        }
    }

    public void close() {
        if (connection != null) {
            try {
                connection.close();
                System.out.println("Connection closed.");
            } catch (SQLException e) {
                System.err.println("Error closing connection: " + e.getMessage());
            }
        }
    }


    public static void main(String[] args) {
        HiveKerberosConnection hiveConn = new HiveKerberosConnection();

        try {
            hiveConn.connect();

            System.out.println("\n=== SELECT Query ===");
            hiveConn.executeQuery("SELECT * FROM fmmis_sri_open.razor_dvp_nastro_fx_view_v2 LIMIT 3");

            System.out.println("\n=== INSERT OVERWRITE Statement ===");
            String insertSQL = "INSERT OVERWRITE TABLE fmmis_sri_open.Razor_ztp_mm_final PARTITION(fmmis_part_date='20230101') " +
                    "select category,cnt_origin,trn_no,cnt_version,trn_date,start_date,next_pay_date,mature_date,notional_amount,typology,entity,counterparty,cpty_lname,stlmethod,ctp_type,ctp_fmid,conf_medium,actionowner,actiongroup,actionplace,actionname,actiondate,actionreason,mainstatus,substatus,conf_comment,sett_comment,sett_freetext,dlv_flow_id,dlv_send_receive,lienamount,gle_grp_id,p100,sci_counterparty_type,'','','' from fmmis_landing.Razor_ztp_mm_final_history_confirm where fmmis_part_date=20240101 limit 1";
            hiveConn.executeUpdate(insertSQL);

            System.out.println("\n=== ALTER TABLE Statement ===");
            String alterSQL = "ALTER table fmmis_storage.PCT2_PORTFOLIOPROFILE add  IF NOT EXISTS partition(fmmis_part_date='20250616') Location '/apps/fmoapp/hdata/fmmis_storage/pct2/all/avro/pct2_portfolioprofile/fmmis_part_date=20250616'";
            hiveConn.executeDDL(alterSQL);

        } catch (Exception e) {
            System.err.println("Application error: " + e.getMessage());
            e.printStackTrace();
        } finally {
            hiveConn.close();
        }
    }

}
